{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e435bfa3",
   "metadata": {},
   "source": [
    "# Fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c6a109",
   "metadata": {},
   "source": [
    "## Création du premier dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b6e1f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def creer_dataset_initial_depuis_s3(bucket):\n",
    "    colonnes = [\"chemin_image\", \"label\"]\n",
    "    valeurs = recuperer_valeurs_dataset_depuis_s3(bucket, True)\n",
    "    return spark.createDataFrame(valeurs, colonnes)\n",
    "\n",
    "def recuperer_valeurs_dataset_depuis_s3(bucket, utiliser_echantillon):\n",
    "    valeurs = []\n",
    "    \n",
    "    if utiliser_echantillon:\n",
    "        expression_image = \"echantillon.*\\.jpg\"\n",
    "    else:\n",
    "        expression_image = \"dataset.*\\.jpg\"\n",
    "    \n",
    "    for objet_bucket in bucket.objects.all():\n",
    "        if re.search(expression_image, objet_bucket.key):\n",
    "            chemin_image = objet_bucket.key\n",
    "            categorie_image = recuperer_categorie_image_depuis_s3(objet_bucket.key)\n",
    "            valeurs.append([chemin_image, categorie_image])\n",
    "            \n",
    "    return valeurs\n",
    "\n",
    "def recuperer_categorie_image_depuis_s3(cle_objet_bucket):\n",
    "    liste_dossiers_fichiers = cle_objet_bucket.split('/')\n",
    "    categorie_fruit = liste_dossiers_fichiers[1]\n",
    "    categorie_fruit = categorie_fruit.lower()\n",
    "    categorie_fruit = categorie_fruit.replace(\" \", \"_\")\n",
    "    return categorie_fruit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ba977e",
   "metadata": {},
   "source": [
    "## Extraction des features automatiquement depuis S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea939fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.utils import load_img, img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "def extraire_features_depuis_s3(chemin_image):\n",
    "    \n",
    "    # Récupération du bucket\n",
    "    bucket_p8 = recuperer_bucket_s3()\n",
    "    objet = bucket_p8.Object(chemin_image)\n",
    "    file_binary_stream = io.BytesIO()\n",
    "    objet.download_fileobj(file_binary_stream)\n",
    "    \n",
    "    # Chargement et prétraitement de l'image\n",
    "    image = load_img(file_binary_stream, target_size=(224, 224))\n",
    "    \n",
    "    # Convertir en tableau numpy\n",
    "    image = img_to_array(image) \n",
    "    \n",
    "    # Créer la collection d'images (un seul échantillon)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    \n",
    "    # Prétraiter l'image comme le veut VGG-16\n",
    "    image = preprocess_input(image) \n",
    "    \n",
    "    # Extraire les features de l'image\n",
    "    cnn_extraction_features = creer_cnn_extraction_features()\n",
    "    features = cnn_extraction_features.predict(image)[0]\n",
    "    features = features.tolist()\n",
    "    \n",
    "    file_binary_stream.close()\n",
    "    \n",
    "    return features\n",
    "    \n",
    "def recuperer_bucket_s3():\n",
    "    \n",
    "    # Récupération des clés d'accès AWS\n",
    "    access_key_id, secret_access_key = recuperer_cles_acces_aws()\n",
    "    \n",
    "    # Connexion à S3\n",
    "    s3 = boto3.resource(\n",
    "        service_name='s3',\n",
    "        region_name='eu-west-3',\n",
    "        aws_access_key_id=access_key_id,\n",
    "        aws_secret_access_key=secret_access_key\n",
    "    )\n",
    "    \n",
    "    return s3.Bucket(\"oc-p8-sb-data\")\n",
    "\n",
    "def recuperer_cles_acces_aws():\n",
    "    fichier_access_keys_AWS = \"rootkey.csv\"\n",
    "    df_access_keys_AWS = pd.read_csv(fichier_access_keys_AWS)\n",
    "    access_key_id = df_access_keys_AWS[\"AWSAccessKeyId\"][0]\n",
    "    secret_access_key = df_access_keys_AWS[\"AWSSecretKey\"][0]\n",
    "    return access_key_id, secret_access_key\n",
    "\n",
    "def creer_cnn_extraction_features():\n",
    "    # Initialisation du modèle VGG-16 et suppression de ses 2 dernières couches (prédiction et fully-connected)\n",
    "    cnn = VGG16()\n",
    "    cnn = Model(inputs=cnn.inputs, outputs=cnn.layers[-2].output)\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09800c47",
   "metadata": {},
   "source": [
    "## Extraction des features \"à la main\" depuis S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a274656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.utils import load_img, img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "def creer_cnn_extraction_features():\n",
    "    # Initialisation du modèle VGG-16 et suppression de ses 2 dernières couches (prédiction et fully-connected)\n",
    "    cnn = VGG16()\n",
    "    cnn = Model(inputs=cnn.inputs, outputs=cnn.layers[-2].output)\n",
    "    return cnn\n",
    "\n",
    "def extraire_features_depuis_s3(chemin_image):\n",
    "    \n",
    "    # Récupération du bucket\n",
    "    objet = bucket.Object(chemin_image)\n",
    "    file_binary_stream = io.BytesIO()\n",
    "    objet.download_fileobj(file_binary_stream)\n",
    "    \n",
    "    # Chargement et prétraitement de l'image\n",
    "    image = load_img(file_binary_stream, target_size=(224, 224))\n",
    "    \n",
    "    # Convertir en tableau numpy\n",
    "    image = img_to_array(image) \n",
    "    \n",
    "    # Créer la collection d'images (un seul échantillon)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    \n",
    "    # Prétraiter l'image comme le veut VGG-16\n",
    "    image = preprocess_input(image) \n",
    "    \n",
    "    # Extraire les features de l'image\n",
    "    features = cnn_extraction_features.predict(image)[0]\n",
    "    features = features.tolist()\n",
    "    \n",
    "    file_binary_stream.close()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf80da4",
   "metadata": {},
   "source": [
    "## Choix du nombre optimum de composantes principales pour l'ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49dfaddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "def choisir_nombre_optimum_composantes_principales(df_images_fruits):\n",
    "    nombres_composantes_principales = [100, 150, 200, 225]\n",
    "    variance_expliquee = []\n",
    "    \n",
    "    # Récupération des variances expliquées pour chaque nombre de composantes principales\n",
    "    for nombre_composantes_principales in nombres_composantes_principales:\n",
    "        print(\"Nombre de CP étudié : {}\".format(nombre_composantes_principales))\n",
    "        acp = PCA(k=nombre_composantes_principales, inputCol=\"features_vector\", outputCol=\"resultats_acp_vector\")\n",
    "        acp = acp.fit(df_images_fruits)\n",
    "        variance_expliquee.append(acp.explainedVariance.sum())\n",
    "    \n",
    "    # Recherche du premier index atteignant 95% de variance expliquée\n",
    "    index = 0\n",
    "    seuil_atteint = False\n",
    "    taille_listes = len(variance_expliquee)\n",
    "    while index < taille_listes and not seuil_atteint:\n",
    "        if variance_expliquee[index] >= 0.95:\n",
    "            seuil_atteint = True\n",
    "        else:\n",
    "            index += 1\n",
    "    \n",
    "    # Renvoi du nombre optimum de composantes principales selon si le seuil a été atteint ou non\n",
    "    if seuil_atteint:\n",
    "        return nombres_composantes_principales[index]\n",
    "    else:\n",
    "        return nombres_composantes_principales[taille_listes-1]        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17728880",
   "metadata": {},
   "source": [
    "## Export des résultats vers S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d072985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exporter_resultats_acp_vers_s3(df_pandas_resultat_acp):\n",
    "    file_binary_stream = io.BytesIO()\n",
    "    df_pandas_resultat_acp.to_csv(file_binary_stream, mode=\"wb\", header=True, sep=\";\", index=False)\n",
    "    file_binary_stream.seek(0)\n",
    "    bucket.upload_fileobj(file_binary_stream, \"sortie_acp.csv\")\n",
    "    file_binary_stream.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377905cd",
   "metadata": {},
   "source": [
    "# Récupération des clés d'accès AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09767bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fichier_access_keys_AWS = \"rootkey.csv\"\n",
    "df_access_keys_AWS = pd.read_csv(fichier_access_keys_AWS)\n",
    "access_key_id = df_access_keys_AWS[\"AWSAccessKeyId\"][0]\n",
    "secret_access_key = df_access_keys_AWS[\"AWSSecretKey\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927adc9c",
   "metadata": {},
   "source": [
    "# Connexion avec S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5566a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name='s3',\n",
    "    region_name='eu-west-3',\n",
    "    aws_access_key_id=access_key_id,\n",
    "    aws_secret_access_key=secret_access_key\n",
    ")\n",
    "\n",
    "bucket = s3.Bucket(\"oc-p8-sb-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9a0cce",
   "metadata": {},
   "source": [
    "# Chargement des images depuis S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcdc7795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|        chemin_image|         label|\n",
      "+--------------------+--------------+\n",
      "|echantillon/Apple...|apple_braeburn|\n",
      "|echantillon/Apple...|apple_braeburn|\n",
      "|echantillon/Apric...|       apricot|\n",
      "|echantillon/Apric...|       apricot|\n",
      "|echantillon/Banan...|        banana|\n",
      "|echantillon/Banan...|        banana|\n",
      "+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "# Initialisation du programme\n",
    "#appName = \"[OC/P8] - Déployer un modèle sur le cloud\"\n",
    "#master = \"local\"\n",
    "#conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "#sc = SparkContext(conf=conf).getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "#spark = SparkSession.builder.config('spark.driver.memory', '4g').getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df_images_fruits = creer_dataset_initial_depuis_s3(bucket)\n",
    "df_images_fruits.persist()\n",
    "df_images_fruits.show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39788068",
   "metadata": {},
   "source": [
    "## Extraction des features des images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a1bab3",
   "metadata": {},
   "source": [
    "## En automatique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d282751",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrateur\\.virtualenvs\\P8_Deployer-modele-cloud-18B3aG_S\\lib\\site-packages\\pyspark\\serializers.py\", line 437, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "  File \"C:\\Users\\Administrateur\\.virtualenvs\\P8_Deployer-modele-cloud-18B3aG_S\\lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 72, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"C:\\Users\\Administrateur\\.virtualenvs\\P8_Deployer-modele-cloud-18B3aG_S\\lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 540, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "TypeError: cannot pickle '_thread.lock' object\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: TypeError: cannot pickle '_thread.lock' object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\.virtualenvs\\P8_Deployer-modele-cloud-18B3aG_S\\lib\\site-packages\\pyspark\\serializers.py:437\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[1;32m~\\.virtualenvs\\P8_Deployer-modele-cloud-18B3aG_S\\lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:72\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[0;32m     69\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[0;32m     70\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[0;32m     71\u001b[0m )\n\u001b[1;32m---> 72\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32m~\\.virtualenvs\\P8_Deployer-modele-cloud-18B3aG_S\\lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:540\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot pickle '_thread.lock' object",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m udf\n\u001b[0;32m      4\u001b[0m udf_extraire_features \u001b[38;5;241m=\u001b[39m udf(extraire_features_depuis_s3, ArrayType(FloatType()))\n\u001b[1;32m----> 6\u001b[0m df_images_fruits \u001b[38;5;241m=\u001b[39m df_images_fruits\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mudf_extraire_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchemin_image\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      7\u001b[0m df_images_fruits\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\.virtualenvs\\P8_Deployer-modele-cloud-18B3aG_S\\lib\\site-packages\\pyspark\\sql\\udf.py:199\u001b[0m, in \u001b[0;36mUserDefinedFunction._wrapped.<locals>.wrapper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, assigned\u001b[38;5;241m=\u001b[39massignments)\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\P8_Deployer-modele-cloud-18B3aG_S\\lib\\site-packages\\pyspark\\sql\\udf.py:177\u001b[0m, in \u001b[0;36mUserDefinedFunction.__call__\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols):\n\u001b[1;32m--> 177\u001b[0m     judf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_judf\u001b[49m\n\u001b[0;32m    178\u001b[0m     sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Column(judf\u001b[38;5;241m.\u001b[39mapply(_to_seq(sc, cols, _to_java_column)))\n",
      "File \u001b[1;32m~\\.virtualenvs\\P8_Deployer-modele-cloud-18B3aG_S\\lib\\site-packages\\pyspark\\sql\\udf.py:161\u001b[0m, in \u001b[0;36mUserDefinedFunction._judf\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_judf\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# It is possible that concurrent access, to newly created UDF,\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# will initialize multiple UserDefinedPythonFunctions.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# This is unlikely, doesn't affect correctness,\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# and should have a minimal performance impact.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf_placeholder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf_placeholder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_judf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf_placeholder\n",
      "File \u001b[1;32m~\\.virtualenvs\\P8_Deployer-modele-cloud-18B3aG_S\\lib\\site-packages\\pyspark\\sql\\udf.py:170\u001b[0m, in \u001b[0;36mUserDefinedFunction._create_judf\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    167\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m    168\u001b[0m sc \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\n\u001b[1;32m--> 170\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturnType\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m jdt \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mparseDataType(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturnType\u001b[38;5;241m.\u001b[39mjson())\n\u001b[0;32m    172\u001b[0m judf \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mexecution\u001b[38;5;241m.\u001b[39mpython\u001b[38;5;241m.\u001b[39mUserDefinedPythonFunction(\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, wrapped_func, jdt, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevalType, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterministic)\n",
      "File \u001b[1;32m~\\.virtualenvs\\P8_Deployer-modele-cloud-18B3aG_S\\lib\\site-packages\\pyspark\\sql\\udf.py:34\u001b[0m, in \u001b[0;36m_wrap_function\u001b[1;34m(sc, func, returnType)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_function\u001b[39m(sc, func, returnType):\n\u001b[0;32m     33\u001b[0m     command \u001b[38;5;241m=\u001b[39m (func, returnType)\n\u001b[1;32m---> 34\u001b[0m     pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonFunction(\u001b[38;5;28mbytearray\u001b[39m(pickled_command), env, includes, sc\u001b[38;5;241m.\u001b[39mpythonExec,\n\u001b[0;32m     36\u001b[0m                                   sc\u001b[38;5;241m.\u001b[39mpythonVer, broadcast_vars, sc\u001b[38;5;241m.\u001b[39m_javaAccumulator)\n",
      "File \u001b[1;32m~\\.virtualenvs\\P8_Deployer-modele-cloud-18B3aG_S\\lib\\site-packages\\pyspark\\rdd.py:2814\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[1;34m(sc, command)\u001b[0m\n\u001b[0;32m   2811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc, command):\n\u001b[0;32m   2812\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[0;32m   2813\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[1;32m-> 2814\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m \u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[0;32m   2816\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n\u001b[0;32m   2817\u001b[0m         broadcast \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mbroadcast(pickled_command)\n",
      "File \u001b[1;32m~\\.virtualenvs\\P8_Deployer-modele-cloud-18B3aG_S\\lib\\site-packages\\pyspark\\serializers.py:447\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    445\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[0;32m    446\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m--> 447\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[1;31mPicklingError\u001b[0m: Could not serialize object: TypeError: cannot pickle '_thread.lock' object"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "#    ATTENTION : à ne faire tourner que pour tester     #\n",
    "#########################################################\n",
    "from pyspark.sql.types import StructType, ArrayType, FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "udf_extraire_features = udf(extraire_features_depuis_s3, ArrayType(FloatType()))\n",
    "\n",
    "df_images_fruits = df_images_fruits.withColumn(\"features\", udf_extraire_features(\"chemin_image\"))\n",
    "df_images_fruits.show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494f257e",
   "metadata": {},
   "source": [
    "## \"À la main\" pour éviter de faire planter le PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0ffad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_extraction_features = creer_cnn_extraction_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65dbbd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "features_1 = extraire_features_depuis_s3('echantillon/Apple Braeburn/0_100.jpg')\n",
    "features_2 = extraire_features_depuis_s3('echantillon/Apple Braeburn/1_100.jpg')\n",
    "features_3 = extraire_features_depuis_s3('echantillon/Apricot/0_100.jpg')\n",
    "features_4 = extraire_features_depuis_s3('echantillon/Apricot/1_100.jpg')\n",
    "features_5 = extraire_features_depuis_s3('echantillon/Banana/0_100.jpg')\n",
    "features_6 = extraire_features_depuis_s3('echantillon/Banana/1_100.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "345f0148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        chemin_image|            features|\n",
      "+--------------------+--------------------+\n",
      "|echantillon/Apple...|[0.0, 0.0, 0.0, 0...|\n",
      "|echantillon/Apple...|[0.0, 0.0, 0.0, 0...|\n",
      "|echantillon/Apric...|[0.0, 0.0, 0.0, 0...|\n",
      "|echantillon/Apric...|[0.0, 0.0, 0.0, 0...|\n",
      "|echantillon/Banan...|[0.0, 0.0, 0.0, 1...|\n",
      "|echantillon/Banan...|[0.0, 0.0, 0.3409...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "rdd_features = sc.parallelize([Row(chemin_image=\"echantillon/Apple Braeburn/0_100.jpg\", features=features_1), \n",
    "                               Row(chemin_image=\"echantillon/Apple Braeburn/1_100.jpg\", features=features_2), \n",
    "                               Row(chemin_image=\"echantillon/Apricot/0_100.jpg\", features=features_3), \n",
    "                               Row(chemin_image=\"echantillon/Apricot/1_100.jpg\", features=features_4), \n",
    "                               Row(chemin_image=\"echantillon/Banana/0_100.jpg\", features=features_5), \n",
    "                               Row(chemin_image=\"echantillon/Banana/1_100.jpg\", features=features_6)])\n",
    "df_features = spark.createDataFrame(rdd_features)\n",
    "df_features.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6a1c8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+\n",
      "|        chemin_image|         label|            features|\n",
      "+--------------------+--------------+--------------------+\n",
      "|echantillon/Apple...|apple_braeburn|[0.0, 0.0, 0.0, 0...|\n",
      "|echantillon/Apple...|apple_braeburn|[0.0, 0.0, 0.0, 0...|\n",
      "|echantillon/Apric...|       apricot|[0.0, 0.0, 0.0, 0...|\n",
      "|echantillon/Apric...|       apricot|[0.0, 0.0, 0.0, 0...|\n",
      "|echantillon/Banan...|        banana|[0.0, 0.0, 0.0, 1...|\n",
      "|echantillon/Banan...|        banana|[0.0, 0.0, 0.3409...|\n",
      "+--------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_images_fruits = df_images_fruits.join(df_features, \"chemin_image\").sort(\"chemin_image\")\n",
    "df_images_fruits.persist()\n",
    "df_images_fruits.show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4503e7da",
   "metadata": {},
   "source": [
    "# Réduction de dimension avec l'ACP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaada15b",
   "metadata": {},
   "source": [
    "## Conversion des features en Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39ec4011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "udf_convertir_en_vecteur = udf(lambda x: Vectors.dense(x), VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf6e0da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+--------------------+\n",
      "|        chemin_image|         label|            features|     features_vector|\n",
      "+--------------------+--------------+--------------------+--------------------+\n",
      "|echantillon/Apple...|apple_braeburn|[0.0, 0.0, 0.0, 0...|[0.0,0.0,0.0,0.0,...|\n",
      "|echantillon/Apple...|apple_braeburn|[0.0, 0.0, 0.0, 0...|[0.0,0.0,0.0,0.0,...|\n",
      "|echantillon/Apric...|       apricot|[0.0, 0.0, 0.0, 0...|[0.0,0.0,0.0,0.0,...|\n",
      "|echantillon/Apric...|       apricot|[0.0, 0.0, 0.0, 0...|[0.0,0.0,0.0,0.0,...|\n",
      "|echantillon/Banan...|        banana|[0.0, 0.0, 0.0, 1...|[0.0,0.0,0.0,1.30...|\n",
      "|echantillon/Banan...|        banana|[0.0, 0.0, 0.3409...|[0.0,0.0,0.340979...|\n",
      "+--------------------+--------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_images_fruits = df_images_fruits.withColumn('features_vector', udf_convertir_en_vecteur(\"features\"))\n",
    "df_images_fruits.persist()\n",
    "df_images_fruits.show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fa86ac",
   "metadata": {},
   "source": [
    "## ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "495a8e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de CP étudié : 100\n",
      "Nombre de CP étudié : 150\n",
      "Nombre de CP étudié : 200\n",
      "Nombre de CP étudié : 225\n"
     ]
    }
   ],
   "source": [
    "nombre_optimum_composantes_principales = choisir_nombre_optimum_composantes_principales(df_images_fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ae8486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre optimum de CP : 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre optimum de CP : {}\".format(nombre_optimum_composantes_principales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70d2f0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "acp = PCA(k=nombre_optimum_composantes_principales, inputCol=\"features_vector\", outputCol=\"resultats_acp_vector\")\n",
    "acp = acp.fit(df_images_fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff4af9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "|        chemin_image|         label|            features|     features_vector|resultats_acp_vector|\n",
      "+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "|echantillon/Apple...|apple_braeburn|[0.0, 0.0, 0.0, 0...|[0.0,0.0,0.0,0.0,...|[47.8794401749720...|\n",
      "|echantillon/Apple...|apple_braeburn|[0.0, 0.0, 0.0, 0...|[0.0,0.0,0.0,0.0,...|[49.1322660292027...|\n",
      "|echantillon/Apric...|       apricot|[0.0, 0.0, 0.0, 0...|[0.0,0.0,0.0,0.0,...|[29.8963130558993...|\n",
      "|echantillon/Apric...|       apricot|[0.0, 0.0, 0.0, 0...|[0.0,0.0,0.0,0.0,...|[30.9112918822341...|\n",
      "|echantillon/Banan...|        banana|[0.0, 0.0, 0.0, 1...|[0.0,0.0,0.0,1.30...|[-44.434613084136...|\n",
      "|echantillon/Banan...|        banana|[0.0, 0.0, 0.3409...|[0.0,0.0,0.340979...|[-42.526938587708...|\n",
      "+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_images_fruits = acp.transform(df_images_fruits)\n",
    "df_images_fruits.persist()\n",
    "df_images_fruits.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c5ab238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de variance expliquée : 1.0000000000000027\n"
     ]
    }
   ],
   "source": [
    "print('Taux de variance expliquée :', acp.explainedVariance.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c592507",
   "metadata": {},
   "source": [
    "# Export du résultat de la réduction de dimension vers S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cf2a340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+\n",
      "|        chemin_image|         label|resultats_acp_vector|\n",
      "+--------------------+--------------+--------------------+\n",
      "|echantillon/Apple...|apple_braeburn|[47.8794401749720...|\n",
      "|echantillon/Apple...|apple_braeburn|[49.1322660292027...|\n",
      "|echantillon/Apric...|       apricot|[29.8963130558993...|\n",
      "|echantillon/Apric...|       apricot|[30.9112918822341...|\n",
      "|echantillon/Banan...|        banana|[-44.434613084136...|\n",
      "|echantillon/Banan...|        banana|[-42.526938587708...|\n",
      "+--------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_resultat_acp = df_images_fruits.select(\"chemin_image\", \"label\", \"resultats_acp_vector\")\n",
    "df_resultat_acp.persist()\n",
    "df_resultat_acp.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "657d037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas_resultat_acp = df_resultat_acp.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7203a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exporter_resultats_acp_vers_s3(df_pandas_resultat_acp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oc-p8-deployer-modele-cloud",
   "language": "python",
   "name": "oc-p8-deployer-modele-cloud"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
